ACTIVITY 1:

basic implementations

What are the (simulated) wall-clock time of the three implementations on the 50-processor ring?

How far are your "by hand" implementations from the default broadcast?

You may observe that ring_bcast does not improve a lot over naive_bcast, which should be surprising to you. After all, naive_bcast sends long-haul messages while ring_bcast doesn't What do you think the reason is? 
 - To answer this question, you can INSTRUMENT MY CODE and run it on SMALLER RINGS to see when events happen and try to understand what's going on. 
 - Given that we're using simulation, you should take advantage of it and experiment with all kinds of platform configurations to gain understanding of the performance behavior. For instance, you can MODIFY LINK LATENCIES and BANDWIDTHS. 
 - The MPI_Wtime function is convenient to determine the current (simulated) date. This function returns the date as a double precision number (and is in fact already used in bcast_skeleton.c).

  DO 50-ring and 5-ring

   ----------> Increase the latency for each thing like crazy

   ----------> Increase the latency for each thing like crazy AND Decrease the hell out of the bandwidth (to be only a few buffer sizes?)

 TODO:
  - learn how to use the python script

----------------------------------------------------------------------------------------------------------------------------------------------------

ACTIVITY 2:

platforms: 20-processor ring, a 35-processor ring, and a 50-processor ring
chunk sizes: 100,000 bytes, 500,000 bytes, 1,000,000 bytes, 5,000,000 bytes, 10,000,000 bytes, 50,000,000 bytes, and 100,000,000 (i.e., 1 chunk)

NOTE: I'm doing a a power of ten lower for my NUM_BYTES because smpi was crashing... I hope this is okay.


What is the best chunk size for each of the three platforms?

Does this chunk size depend on the platform size?

You should observe that the wall-clock time is minimized for a chunk size that is neither the smallest nor the largest. Discuss why you think that is.

For a 100-processor ring, with the best chunk size determined above, by how much does pipelining help when compared to using a single chunk?
Do outside of test script --> custom result file

How does the performance compare to that of the default MPI broadcast for that platform as seen in the previous question?
Do outside as well  --> custom result file

What do you conclude about the use of pipelined communications for the purpose of a ring-based broadcast on a ring-shaped physical platform?



----------------------------------------------------------------------------------------------------------------------------------------------------

ACTIVITY 3:

platforms: 50-processor ring
chunk sizes: 100,000 bytes, 500,000 bytes, 1,000,000 bytes, 5,000,000 bytes, 10,000,000 bytes, 50,000,000 bytes, and 100,000,000 (i.e., 1 chunk)

Is the best chunk size the same as that for pipelined_ring_bcast?

When using the best chunk size for each of the two implementations (which may be the same if the answer to the previous question is "no"), does the use of MPI_Isend help? By how much?

Is asynchronous_pipelined_ring_bcast faster than default_bcast or slower? By how much?

What do you conclude about the use of asynchronous communications for the purpose of a ring-based broadcast on a ring-shaped physical platform?



